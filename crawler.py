import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import time
import re
from urllib.parse import urlparse

class ShoppingMallCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
    def setup_driver(self):
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        return webdriver.Chrome(options=chrome_options)
    
    def detect_mall_type(self, url):
        """URLì„ ê¸°ë°˜ìœ¼ë¡œ ì‡¼í•‘ëª° íƒ€ì… ê°ì§€"""
        domain = urlparse(url).netloc.lower()
        
        if 'coupang.com' in domain:
            return 'coupang'
        elif 'naver.com' in domain or 'smartstore' in domain:
            return 'naver'
        elif '11st.co.kr' in domain:
            return '11st'
        elif 'gmarket.co.kr' in domain:
            return 'gmarket'
        elif 'auction.co.kr' in domain:
            return 'auction'
        elif 'interpark.com' in domain:
            return 'interpark'
        elif 'dentistestore.kr' in domain:
            # ê²Œì‹œíŒ URLê³¼ ì œí’ˆ URL êµ¬ë¶„
            if '/board/review' in url:
                return 'dentiste_board'
            else:
                return 'dentiste'
        else:
            return 'generic'
    
    def crawl_coupang(self, url):
        """ì¿ íŒ¡ ë¦¬ë·° í¬ë¡¤ë§"""
        reviews = []
        try:
            driver = self.setup_driver()
            driver.get(url)
            time.sleep(3)
            
            # ë¦¬ë·° íƒ­ í´ë¦­
            try:
                review_tab = driver.find_element(By.CSS_SELECTOR, '[data-tab="review"]')
                review_tab.click()
                time.sleep(2)
            except:
                pass
            
            # ë¦¬ë·° ìˆ˜ì§‘
            review_elements = driver.find_elements(By.CSS_SELECTOR, '.sdp-review__article__list__review')
            
            for element in review_elements[:50]:  # ìµœëŒ€ 50ê°œ ë¦¬ë·°
                try:
                    text = element.find_element(By.CSS_SELECTOR, '.sdp-review__article__list__review__content').text
                    rating = len(element.find_elements(By.CSS_SELECTOR, '.sdp-review__article__list__star--active'))
                    
                    reviews.append({
                        'text': text.strip(),
                        'rating': rating,
                        'source': 'coupang'
                    })
                except:
                    continue
                    
            driver.quit()
        except Exception as e:
            print(f"Coupang crawling error: {e}")
            
        return reviews
    
    def crawl_naver(self, url):
        """ë„¤ì´ë²„ ì‡¼í•‘ ë¦¬ë·° í¬ë¡¤ë§"""
        reviews = []
        try:
            response = requests.get(url, headers=self.headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ë¦¬ë·°
            review_elements = soup.find_all('div', class_='review_text')
            
            for element in review_elements[:50]:
                try:
                    text = element.get_text(strip=True)
                    reviews.append({
                        'text': text,
                        'rating': 0,  # ë³„ì ì€ ë³„ë„ ì²˜ë¦¬ í•„ìš”
                        'source': 'naver'
                    })
                except:
                    continue
                    
        except Exception as e:
            print(f"Naver crawling error: {e}")
            
        return reviews
    
    def crawl_11st(self, url):
        """11ë²ˆê°€ ë¦¬ë·° í¬ë¡¤ë§"""
        reviews = []
        try:
            driver = self.setup_driver()
            driver.get(url)
            time.sleep(3)
            
            # ë¦¬ë·° ì„¹ì…˜ìœ¼ë¡œ ìŠ¤í¬ë¡¤
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
            time.sleep(2)
            
            # ë¦¬ë·° ìˆ˜ì§‘
            review_elements = driver.find_elements(By.CSS_SELECTOR, '.review_list_element')
            
            for element in review_elements[:50]:
                try:
                    text = element.find_element(By.CSS_SELECTOR, '.cont_text').text
                    reviews.append({
                        'text': text.strip(),
                        'rating': 0,
                        'source': '11st'
                    })
                except:
                    continue
                    
            driver.quit()
        except Exception as e:
            print(f"11st crawling error: {e}")
            
        return reviews
    
    def crawl_dentiste(self, url):
        """ë´í‹°ìŠ¤í…Œ ìŠ¤í† ì–´ ë¦¬ë·° í¬ë¡¤ë§"""
        reviews = []
        driver = None
        
        try:
            driver = self.setup_driver()
            
            # ëª¨ë“  í˜ì´ì§€ ìˆœíšŒ (1í˜ì´ì§€ë¶€í„° 44í˜ì´ì§€ê¹Œì§€)
            for page_num in range(1, 45):  # 44í˜ì´ì§€ê¹Œì§€
                try:
                    # ê° í˜ì´ì§€ URL ìƒì„± (ë´í‹°ìŠ¤í…ŒëŠ” page_4 íŒŒë¼ë¯¸í„° ì‚¬ìš©)
                    if page_num == 1:
                        page_url = url
                    else:
                        # URLì— page_4 íŒŒë¼ë¯¸í„° ì¶”ê°€
                        separator = '&' if '?' in url else '?'
                        page_url = f"{url}{separator}page_4={page_num}#prdReview"
                    
                    print(f"Dentiste: í˜ì´ì§€ {page_num} í¬ë¡¤ë§ ì¤‘... ({page_url})")
                    
                    driver.get(page_url)
                    time.sleep(3)
                    
                    # ë¦¬ë·° ì„¹ì…˜ìœ¼ë¡œ ìŠ¤í¬ë¡¤
                    try:
                        review_section = driver.find_element(By.CSS_SELECTOR, '.prdReview')
                        driver.execute_script("arguments[0].scrollIntoView();", review_section)
                        time.sleep(2)
                    except:
                        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                        time.sleep(2)
                    
                    # í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ ë¦¬ë·° ì¶”ì¶œ
                    page_source = driver.page_source
                    soup = BeautifulSoup(page_source, 'html.parser')
                    
                    # ë¦¬ë·° ì„¹ì…˜ ì°¾ê¸°
                    review_section = soup.find(class_='prdReview') or soup.find(id='prdReview')
                    if not review_section:
                        print(f"Dentiste: í˜ì´ì§€ {page_num}ì—ì„œ ë¦¬ë·° ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        continue
                    
                    # ë¦¬ë·° ì•„ì´í…œë“¤ ì°¾ê¸°
                    review_items = review_section.find_all('li')
                    page_reviews = 0
                    
                    for item in review_items:
                        try:
                            item_text = item.get_text(strip=True)
                            
                            # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ë¦¬ë·° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            extracted_reviews = self._extract_dentiste_review_text(item_text)
                            
                            for review_text in extracted_reviews:
                                if self._is_valid_dentiste_review(review_text):
                                    reviews.append({
                                        'text': review_text,
                                        'rating': 5,  # ëŒ€ë¶€ë¶„ 5ì  ë¦¬ë·°
                                        'source': 'dentiste'
                                    })
                                    page_reviews += 1
                        
                        except Exception as item_error:
                            continue
                    
                    print(f"Dentiste: í˜ì´ì§€ {page_num}ì—ì„œ {page_reviews}ê°œ ë¦¬ë·° ìˆ˜ì§‘")
                    
                    # ë¦¬ë·°ê°€ ì—†ìœ¼ë©´ í¬ë¡¤ë§ ì¢…ë£Œ
                    if page_reviews == 0:
                        print(f"Dentiste: í˜ì´ì§€ {page_num}ì—ì„œ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ í¬ë¡¤ë§ ì¢…ë£Œ")
                        break
                
                except Exception as page_error:
                    print(f"Dentiste: í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì˜¤ë¥˜ - {page_error}")
                    continue
            
            # ì¤‘ë³µ ì œê±°
            seen = set()
            unique_reviews = []
            for review in reviews:
                if review['text'] not in seen:
                    seen.add(review['text'])
                    unique_reviews.append(review)
            
            print(f"Dentiste: ì´ {len(unique_reviews)}ê°œ ê³ ìœ  ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ")
            return unique_reviews
            
        except Exception as e:
            print(f"Dentiste crawling error: {e}")
            
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
            
        return reviews
    
    def _extract_dentiste_review_text(self, item_text):
        """ë´í‹°ìŠ¤í…Œ ë¦¬ë·°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        extracted = []
        
        # íŒ¨í„´ 1: ëª¨ë“  ë³„ì  ë‹¤ìŒì˜ í…ìŠ¤íŠ¸ (5.0, 4.0, 3.0, 2.0, 1.0)
        pattern1 = r'[1-5]\.0\s+([ê°€-í£\s\.\?!:]{5,200}?)(?=\s*ë”ë³´ê¸°|$|ì´\*+|\d{2}\.\d{2}\.\d{2}|ì¡°íšŒ|ì‹ ê³ )'
        matches1 = re.findall(pattern1, item_text)
        extracted.extend(matches1)
        
        # íŒ¨í„´ 2: ì´ëª¨ì§€ê°€ í¬í•¨ëœ ë¦¬ë·°
        pattern2 = r'([ê°€-í£\s\.\?!:]{5,200}[ğŸ˜€ğŸ˜ŠğŸ‘â¤ï¸ğŸ’•ğŸ˜„ğŸ˜ğŸ¥°ğŸ˜ğŸ™‚â˜ºï¸âœ¨ğŸ’–ğŸ‰][ğŸ˜€ğŸ˜ŠğŸ‘â¤ï¸ğŸ’•ğŸ˜„ğŸ˜ğŸ¥°ğŸ˜ğŸ™‚â˜ºï¸âœ¨ğŸ’–ğŸ‰]*)'
        matches2 = re.findall(pattern2, item_text)
        extracted.extend(matches2)
        
        # íŒ¨í„´ 3: ê¸ì •ì  ê°ì • í‘œí˜„ì´ ìˆëŠ” í…ìŠ¤íŠ¸
        pattern3 = r'([ê°€-í£\s\.\?!:]{5,200}(?:ì¢‹ì•„ìš”|ì¢‹ë„¤ìš”|ë§›ìˆ|ì¶”ì²œ|ë§Œì¡±|ê¸°ë¶„ì´ ì¢‹|ì˜ ì‚°|ë„ˆë¬´ ì¢‹|í¸í•´ìš”|ê¹”ë”|ì²­ê²°|ìµœê³ |ê°ì‚¬|ë¹¨ë¼ìš”)[ê°€-í£\s!.]{0,50})'
        matches3 = re.findall(pattern3, item_text)
        extracted.extend(matches3)
        
        # íŒ¨í„´ 4: ë¶€ì •ì  ê°ì • í‘œí˜„ì´ ìˆëŠ” í…ìŠ¤íŠ¸ (NEW!)
        pattern4 = r'([ê°€-í£\s\.\?!:]{5,200}(?:ë¹„ì‹¸ìš”|ê¹¨ì ¸|í„°ì ¸|ë¬¸ì œ|ë¶ˆí¸|ì‹¤ë§|ì•„ì‰¬|í›„íšŒ|ë³„ë¡œ|ì•ˆ.*ì¢‹|ë‚˜ë¹ |ì˜ëª»|í™•ì¸|ì œë°œ|ì£„ì†¡|ë°°ì†¡|í¬ì¥)[ê°€-í£\s!.]{0,50})'
        matches4 = re.findall(pattern4, item_text)
        extracted.extend(matches4)
        
        # íŒ¨í„´ 5: ë‚ ì§œ ì•ì˜ ì¼ë°˜ì ì¸ í•œê¸€ ë¦¬ë·°
        pattern5 = r'(\d{4}-\d{2}-\d{2})\s+([ê°€-í£\s\.\?!:]{10,200}?)(?=\s*ë”ë³´ê¸°|$|ì‹ ê³ |ì°¨ë‹¨)'
        matches5 = re.findall(pattern5, item_text)
        for match in matches5:
            extracted.append(match[1])  # ë‚ ì§œëŠ” ì œì™¸í•˜ê³  í…ìŠ¤íŠ¸ë§Œ
        
        # íŒ¨í„´ 6: ì¼ë°˜ì ì¸ ë¦¬ë·° íŒ¨í„´ (íŠ¹ìˆ˜ë¬¸ì í¬í•¨)
        pattern6 = r'([ê°€-í£\s\.\?!:]{10,200}?)(?=\s*ë”ë³´ê¸°|$|ì´\*+|\d{2}\.\d{2}\.\d{2}|ì‹ ê³ |ì°¨ë‹¨)'
        matches6 = re.findall(pattern6, item_text)
        extracted.extend(matches6)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        cleaned = []
        for text in extracted:
            text = text.strip()
            if text and len(text) >= 5 and len(text) <= 500:
                # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì¸ì§€ í™•ì¸
                if re.search(r'[ê°€-í£]{3,}', text):  # ìµœì†Œ 3ê¸€ì ì´ìƒ í•œê¸€ í¬í•¨
                    cleaned.append(text)
        
        return list(set(cleaned))
    
    def _is_valid_dentiste_review(self, text):
        """ë´í‹°ìŠ¤í…Œ ë¦¬ë·°ê°€ ìœ íš¨í•œì§€ í™•ì¸"""
        if not text or len(text) < 5 or len(text) > 500:
            return False
        
        # ì œì™¸í•  íŒ¨í„´ë“¤ (ì‹œìŠ¤í…œ ë©”ì‹œì§€ë‚˜ ê³µì§€ì‚¬í•­)
        exclude_patterns = [
            r'ì´ìš©ìì—ê²Œ ì±…ì„',
            r'ìƒí’ˆêµ¬ë§¤ì•ˆë‚´',
            r'ê²°ì œ.*ì•ˆë‚´',
            r'ì¹´ë“œì‚¬ì—ì„œ',
            r'ì„ì˜ë¡œ ì£¼ë¬¸',
            r'ì¬í™”.*ê³µê¸‰',
            r'íœ´ëŒ€ìš©.*í˜„ì¬.*ìœ„ì¹˜',
            r'í™ˆ.*ìƒí’ˆ.*ê´€ì‹¬ìƒí’ˆ',
            r'ì¹´ì¹´ì˜¤í†¡.*ë¼ì¸.*ë°´ë“œ',
            r'íƒ€ì¸ ëª…ì˜',
            r'ë„ë‚œ ì¹´ë“œ',
            r'ê³ ì•¡ê²°ì œ',
            r'ì •ìƒì ì¸ ì£¼ë¬¸',
            r'ì¡°íšŒ\s*\d+$',
            r'^\d+$',
            r'^[1-5]\.0$',
            r'ë”ë³´ê¸°\s*>$',
            r'^ì‹ ê³ .*ì°¨ë‹¨$',
            r'ë´í‹°ìŠ¤í…Œê³µì‹ìŠ¤í† ì–´',
            r'ê³ ê°ì„¼í„°ë¡œ ì—°ë½ì£¼ì„¸ìš”',
            r'100%ë§Œì¡±ì„ ì¶”êµ¬í•˜ëŠ”'
        ]
        
        for pattern in exclude_patterns:
            if re.search(pattern, text):
                return False
        
        # ë¦¬ë·°ë‹¤ìš´ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸ (ê¸ì •/ë¶€ì • ëª¨ë‘ í¬í•¨)
        review_indicators = [
            r'[ğŸ˜€ğŸ˜ŠğŸ‘â¤ï¸ğŸ’•ğŸ˜„ğŸ˜ğŸ¥°ğŸ˜ğŸ™‚â˜ºï¸]',  # ì´ëª¨ì§€
            r'ì¢‹ì•„ìš”|ì¢‹ë„¤ìš”|ë§›ìˆ|ì¶”ì²œ|ë§Œì¡±|ê¸°ë¶„|ì˜.*ì‚°|ë„ˆë¬´.*ì¢‹|í¸í•´ìš”|ê¹”ë”|ì²­ê²°|ë¹¨ë¼ìš”',  # ê¸ì •ì  í‘œí˜„
            r'ë¹„ì‹¸ìš”|ê¹¨ì ¸|í„°ì ¸|ë¬¸ì œ|ë¶ˆí¸|ì‹¤ë§|ì•„ì‰¬|í›„íšŒ|ë³„ë¡œ|ì•ˆ.*ì¢‹|ë‚˜ë¹ |ì˜ëª»|í™•ì¸.*ì•ˆí•˜ì‹œë‚˜ìš”|ì œë°œ',  # ë¶€ì •ì  í‘œí˜„
            r'ì‚¬ìš©|êµ¬ë§¤|ë°°ì†¡|í¬ì¥|ì œí’ˆ|ìƒí’ˆ|ëšœê»‘|ë°•ìŠ¤|í˜¸ìˆ˜|ë°°ì†¡ì§€',  # ì¼ë°˜ì ì¸ ë¦¬ë·° ìš©ì–´
            r'ì¢€.*ë”|í•˜ë‚˜ê°€|ì¤‘.*í•˜ë‚˜|ì–´ë–»ê²Œ|ì™œ|ì–¸ì œ|ì–´ë””|ë­|ëˆ„êµ¬'  # ì§ˆë¬¸/ë¶ˆë§Œ í‘œí˜„
        ]
        
        # ìµœì†Œí•œ í•˜ë‚˜ì˜ ë¦¬ë·° íŒ¨í„´ì´ ìˆì–´ì•¼ í•¨
        has_review_indicator = any(re.search(pattern, text) for pattern in review_indicators)
        
        # ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” í…ìŠ¤íŠ¸ ì œì™¸
        if len(text.strip()) < 8:
            return False
            
        # í•œê¸€ì´ í¬í•¨ë˜ì–´ì•¼ í•¨
        if not re.search(r'[ê°€-í£]{2,}', text):
            return False
        
        return has_review_indicator
    
    def _is_valid_review(self, text):
        """ìœ íš¨í•œ ë¦¬ë·°ì¸ì§€ í™•ì¸"""
        if not text or len(text) < 5 or len(text) > 200:
            return False
        
        # ì œì™¸í•  íŒ¨í„´ë“¤
        exclude_patterns = [
            r'ì´ìš©ìì—ê²Œ ì±…ì„',
            r'ìƒí’ˆêµ¬ë§¤ì•ˆë‚´',
            r'ê²°ì œ.*ì•ˆë‚´',
            r'ë°°ì†¡.*ì§€ì—­',
            r'ì¹´ë“œì‚¬ì—ì„œ',
            r'í™•ì¸ì „í™”',
            r'ì„ì˜ë¡œ ì£¼ë¬¸',
            r'ì¬í™”.*ê³µê¸‰',
            r'íœ´ëŒ€ìš©.*í˜„ì¬.*ìœ„ì¹˜',
            r'í™ˆ.*ìƒí’ˆ.*ê´€ì‹¬ìƒí’ˆ',
            r'ì¹´ì¹´ì˜¤í†¡.*ë¼ì¸.*ë°´ë“œ',
            r'ì•ˆì „ì„ ìœ„í•´',
            r'íƒ€ì¸ ëª…ì˜',
            r'ë„ë‚œ ì¹´ë“œ',
            r'íŒë‹¨ë  ê²½ìš°',
            r'ì†Œìš”ë  ìˆ˜ ìˆìœ¼ë©°',
            r'íƒë°°ì‚¬ì˜ ì‚¬ì •',
            r'ì§€ì—°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤',
            r'ì„œë©´ì„ ë°›ì€',
            r'ì‹œì‘ëœ ë‚ ë¶€í„°',
            r'ê³ ì•¡ê²°ì œ',
            r'ì •ìƒì ì¸ ì£¼ë¬¸',
        ]
        
        for pattern in exclude_patterns:
            if re.search(pattern, text):
                return False
        
        # ë¦¬ë·°ë‹¤ìš´ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸
        review_indicators = [
            r'[ğŸ˜€ğŸ˜ŠğŸ‘â¤ï¸ğŸ’•ğŸ˜„ğŸ˜ğŸ¥°]',  # ì´ëª¨ì§€
            r'ì¢‹ì•„ìš”|ì¢‹ë„¤ìš”|ë§›ìˆ|ì¶”ì²œ|ë§Œì¡±|ê¸°ë¶„|ì˜.*ì‚°',  # ê¸ì •ì  í‘œí˜„
            r'ë³„ë¡œ|ì‹¤ë§|ì•„ì‰¬|ë¶ˆë§Œ|í›„íšŒ',  # ë¶€ì •ì  í‘œí˜„
        ]
        
        has_review_indicator = any(re.search(pattern, text) for pattern in review_indicators)
        
        return has_review_indicator
    
    def crawl_generic(self, url):
        """ì¼ë°˜ ì›¹í˜ì´ì§€ì—ì„œ ë¦¬ë·° ì¶”ì¶œ"""
        reviews = []
        try:
            response = requests.get(url, headers=self.headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì¼ë°˜ì ì¸ ë¦¬ë·° íŒ¨í„´ ì°¾ê¸°
            review_patterns = [
                'review', 'comment', 'ë¦¬ë·°', 'í›„ê¸°', 'í‰ê°€',
                'feedback', 'testimonial', 'ëŒ“ê¸€'
            ]
            
            for pattern in review_patterns:
                # classë‚˜ idì— íŒ¨í„´ì´ í¬í•¨ëœ ìš”ì†Œ ì°¾ê¸°
                elements = soup.find_all(attrs={
                    'class': re.compile(pattern, re.I)
                }) + soup.find_all(attrs={
                    'id': re.compile(pattern, re.I)
                })
                
                for element in elements[:50]:
                    text = element.get_text(strip=True)
                    if len(text) > 10 and len(text) < 1000:  # ì ì ˆí•œ ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ë§Œ
                        reviews.append({
                            'text': text,
                            'rating': 0,
                            'source': 'generic'
                        })
            
            # ì¤‘ë³µ ì œê±°
            seen = set()
            unique_reviews = []
            for review in reviews:
                if review['text'] not in seen:
                    seen.add(review['text'])
                    unique_reviews.append(review)
            
            return unique_reviews[:50]
            
        except Exception as e:
            print(f"Generic crawling error: {e}")
            
        return reviews
    
    def crawl_dentiste_board(self, url, max_pages=3, full_analysis=False, debug=False):
        """ë´í‹°ìŠ¤í…Œ ë¦¬ë·° ê²Œì‹œíŒ í¬ë¡¤ë§ (ì‹ ê·œ ë¦¬ë·° í™•ì¸ìš©)"""
        reviews = []
        driver = None
        
        try:
            driver = self.setup_driver()
            
            print(f"Dentiste Board: ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘ - {url}")
            
            # ìµœê·¼ ëª‡ í˜ì´ì§€ë§Œ í™•ì¸ (ì‹ ê·œ ë¦¬ë·° ëª¨ë‹ˆí„°ë§ìš©)
            for page_num in range(1, max_pages + 1):
                try:
                    # í˜ì´ì§€ URL ìƒì„±
                    if page_num == 1:
                        page_url = url
                    else:
                        separator = '&' if '?' in url else '?'
                        page_url = f"{url}{separator}page={page_num}"
                    
                    print(f"Dentiste Board: í˜ì´ì§€ {page_num} í¬ë¡¤ë§ ì¤‘...")
                    
                    driver.get(page_url)
                    time.sleep(3)
                    
                    # "ë”ë³´ê¸°" ë²„íŠ¼ë“¤ì„ ëª¨ë‘ í´ë¦­í•´ì„œ ì „ì²´ ë¦¬ë·° ë‚´ìš© í™•ì¥
                    try:
                        more_buttons = driver.find_elements(By.XPATH, "//a[contains(text(), 'ë”ë³´ê¸°')]")
                        if debug:
                            print(f"ë”ë³´ê¸° ë²„íŠ¼ {len(more_buttons)}ê°œ ë°œê²¬")
                        
                        for i, button in enumerate(more_buttons[:10]):  # ìµœëŒ€ 10ê°œë§Œ í´ë¦­
                            try:
                                if button.is_displayed():
                                    driver.execute_script("arguments[0].click();", button)
                                    time.sleep(0.5)  # ì§§ì€ ëŒ€ê¸°
                                    if debug:
                                        print(f"ë”ë³´ê¸° ë²„íŠ¼ {i+1} í´ë¦­ ì™„ë£Œ")
                            except Exception as click_error:
                                if debug:
                                    print(f"ë”ë³´ê¸° ë²„íŠ¼ {i+1} í´ë¦­ ì‹¤íŒ¨: {click_error}")
                                continue
                        
                        # ëª¨ë“  ë”ë³´ê¸° í´ë¦­ í›„ ì ì‹œ ëŒ€ê¸°
                        time.sleep(2)
                        
                    except Exception as expand_error:
                        if debug:
                            print(f"ë”ë³´ê¸° í™•ì¥ ì˜¤ë¥˜: {expand_error}")
                    
                    # í™•ì¥ëœ í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ ë¦¬ë·° ì¶”ì¶œ
                    page_source = driver.page_source
                    soup = BeautifulSoup(page_source, 'html.parser')
                    
                    # ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
                    review_items = soup.find_all('li')
                    page_reviews = 0
                    
                    for i, item in enumerate(review_items):
                        try:
                            item_text = item.get_text(strip=True)
                            
                            if debug and item_text and len(item_text) > 10:
                                print(f"\n--- ì•„ì´í…œ {i+1} (í˜ì´ì§€ {page_num}) ---")
                                print(f"ì›ë³¸ í…ìŠ¤íŠ¸: {item_text[:200]}...")
                            
                            # ê²Œì‹œíŒ ìŠ¤íƒ€ì¼ ë¦¬ë·° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            extracted_reviews = self._extract_board_review_text(item_text)
                            
                            if debug and extracted_reviews:
                                print(f"ì¶”ì¶œëœ ë¦¬ë·°ë“¤: {len(extracted_reviews)}ê°œ")
                                for j, extracted in enumerate(extracted_reviews):
                                    print(f"  {j+1}. '{extracted}'")
                            
                            for review_text in extracted_reviews:
                                is_valid = self._is_valid_board_review(review_text)
                                
                                if debug:
                                    print(f"  â†’ ìœ íš¨ì„± ê²€ì‚¬: {'âœ… í†µê³¼' if is_valid else 'âŒ ì‹¤íŒ¨'}")
                                
                                if is_valid:
                                    # ë‚ ì§œ ì¶”ì¶œ ì‹œë„
                                    date_match = re.search(r'(\d{4}-\d{2}-\d{2})', item_text)
                                    review_date = date_match.group(1) if date_match else None
                                    
                                    reviews.append({
                                        'text': review_text,
                                        'rating': 5,  # ê²Œì‹œíŒì—ì„œëŠ” ëŒ€ë¶€ë¶„ ê¸ì •ì 
                                        'source': 'dentiste_board',
                                        'date': review_date,
                                        'page': page_num
                                    })
                                    page_reviews += 1
                        
                        except Exception as item_error:
                            if debug:
                                print(f"ì•„ì´í…œ ì²˜ë¦¬ ì˜¤ë¥˜: {item_error}")
                            continue
                    
                    print(f"Dentiste Board: í˜ì´ì§€ {page_num}ì—ì„œ {page_reviews}ê°œ ë¦¬ë·° ìˆ˜ì§‘")
                    
                    # ë¦¬ë·°ê°€ ì—†ìœ¼ë©´ ë” ì´ìƒ ì§„í–‰í•˜ì§€ ì•ŠìŒ
                    if page_reviews == 0:
                        print(f"Dentiste Board: í˜ì´ì§€ {page_num}ì—ì„œ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ í¬ë¡¤ë§ ì¢…ë£Œ")
                        break
                
                except Exception as page_error:
                    print(f"Dentiste Board: í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì˜¤ë¥˜ - {page_error}")
                    continue
            
            # ì¤‘ë³µ ì œê±°
            seen = set()
            unique_reviews = []
            duplicates = []
            
            for review in reviews:
                if review['text'] not in seen:
                    seen.add(review['text'])
                    unique_reviews.append(review)
                else:
                    duplicates.append(review)
            
            if debug:
                print(f"\n=== ì¤‘ë³µ ì œê±° ê²°ê³¼ ===")
                print(f"ì´ ìˆ˜ì§‘: {len(reviews)}ê°œ")
                print(f"ê³ ìœ  ë¦¬ë·°: {len(unique_reviews)}ê°œ")
                print(f"ì¤‘ë³µ ë¦¬ë·°: {len(duplicates)}ê°œ")
                
                if duplicates:
                    print(f"\nì¤‘ë³µëœ ë¦¬ë·°ë“¤:")
                    dup_texts = {}
                    for dup in duplicates:
                        text = dup['text'][:50] + "..."
                        dup_texts[text] = dup_texts.get(text, 0) + 1
                    
                    for text, count in dup_texts.items():
                        print(f"  '{text}' - {count}ë²ˆ ì¤‘ë³µ")
                
                print(f"\nê³ ìœ  ë¦¬ë·° ëª©ë¡:")
                for i, review in enumerate(unique_reviews, 1):
                    print(f"  {i}. [{review.get('page', '?')}í˜ì´ì§€] {review['text'][:60]}...")
            
            print(f"Dentiste Board: ì´ {len(unique_reviews)}ê°œ ê³ ìœ  ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ")
            return unique_reviews
            
        except Exception as e:
            print(f"Dentiste Board crawling error: {e}")
            
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
            
        return reviews
    
    def _extract_board_review_text(self, item_text):
        """ê²Œì‹œíŒ ë¦¬ë·°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë”ë³´ê¸° í™•ì¥ í›„)"""
        extracted = []
        
        # íŒ¨í„´ 1: ë³„ì  ë‹¤ìŒì˜ ë¦¬ë·° í…ìŠ¤íŠ¸ (ë”ë³´ê¸° í´ë¦­ í›„ í™•ì¥ëœ ë‚´ìš©)
        pattern1 = r'[1-5]\.0.*?ì¶”ì²œ\d+\s*([ê°€-í£\s\.\?!:ğŸ˜€ğŸ˜ŠğŸ‘â¤ï¸ğŸ’•ğŸ˜„ğŸ˜ğŸ¥°ğŸ˜ğŸ™‚â˜ºï¸âœ¨ğŸ’–ğŸ‰ğŸŒˆğŸ€]{10,500}?)(?=ë”ë³´ê¸°|ì¡°íšŒ|\d{2}\.\d{2}\.\d{2}|$)'
        matches1 = re.findall(pattern1, item_text, re.DOTALL)
        extracted.extend(matches1)
        
        # íŒ¨í„´ 2: ë‚ ì§œ ì•ì˜ ë¦¬ë·° í…ìŠ¤íŠ¸
        pattern2 = r'([ê°€-í£\s\.\?!:ğŸ˜€ğŸ˜ŠğŸ‘â¤ï¸ğŸ’•ğŸ˜„ğŸ˜ğŸ¥°ğŸ˜ğŸ™‚â˜ºï¸âœ¨ğŸ’–ğŸ‰ğŸŒˆğŸ€]{15,500}?)\s*[ê°€-í£]\*+\s*\d{2}\.\d{2}\.\d{2}'
        matches2 = re.findall(pattern2, item_text)
        extracted.extend(matches2)
        
        # íŒ¨í„´ 3: ê°ì • í‘œí˜„ì´ í¬í•¨ëœ ê¸´ í…ìŠ¤íŠ¸ (í™•ì¥ëœ ë¦¬ë·°)
        pattern3 = r'([ê°€-í£\s\.\?!:ğŸ˜€ğŸ˜ŠğŸ‘â¤ï¸ğŸ’•ğŸ˜„ğŸ˜ğŸ¥°ğŸ˜ğŸ™‚â˜ºï¸âœ¨ğŸ’–ğŸ‰ğŸŒˆğŸ€]{20,500}(?:ì¢‹ì•„ìš”|ì¢‹ë„¤ìš”|ë§Œì¡±|ì¶”ì²œ|ê°ì‚¬|ê¸°ë¶„|ì˜.*ì‚°|ë„ˆë¬´.*ì¢‹|í¸í•´ìš”|ê¹”ë”|ì²­ê²°|ìµœê³ |ì‚¬ìš©|êµ¬ë§¤|ë°°ì†¡|í¬ì¥|ì œí’ˆ|ìƒí’ˆ|ë¹„ì‹¸ìš”|ê¹¨ì ¸|ë¬¸ì œ|ë¶ˆí¸|ì‹¤ë§|ë³„ë¡œ|ì•ˆ.*ì¢‹)[ê°€-í£\s\.\?!:ğŸ˜€ğŸ˜ŠğŸ‘â¤ï¸ğŸ’•ğŸ˜„ğŸ˜ğŸ¥°ğŸ˜ğŸ™‚â˜ºï¸âœ¨ğŸ’–ğŸ‰ğŸŒˆğŸ€]{0,100})'
        matches3 = re.findall(pattern3, item_text)
        extracted.extend(matches3)
        
        # íŒ¨í„´ 4: ì´ëª¨ì§€ê°€ í¬í•¨ëœ ë¦¬ë·° (í™•ì¥ ë²„ì „)
        pattern4 = r'([ê°€-í£\s\.\?!:]{10,500}[ğŸ˜€ğŸ˜ŠğŸ‘â¤ï¸ğŸ’•ğŸ˜„ğŸ˜ğŸ¥°ğŸ˜ğŸ™‚â˜ºï¸âœ¨ğŸ’–ğŸ‰ğŸŒˆğŸ€][ê°€-í£\s\.\?!:ğŸ˜€ğŸ˜ŠğŸ‘â¤ï¸ğŸ’•ğŸ˜„ğŸ˜ğŸ¥°ğŸ˜ğŸ™‚â˜ºï¸âœ¨ğŸ’–ğŸ‰ğŸŒˆğŸ€]*[ê°€-í£\s\.\?!:]{0,50})'
        matches4 = re.findall(pattern4, item_text)
        extracted.extend(matches4)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        cleaned = []
        for text in extracted:
            text = re.sub(r'\s+', ' ', text.strip())  # ê³µë°± ì •ë¦¬
            # "ë”ë³´ê¸°" ì œê±°
            text = re.sub(r'ë”ë³´ê¸°\s*>', '', text)
            
            if text and len(text) >= 10 and len(text) <= 1000:
                if re.search(r'[ê°€-í£]{5,}', text):  # ìµœì†Œ 5ê¸€ì ì´ìƒ í•œê¸€ í¬í•¨
                    # ë„ˆë¬´ ë°˜ë³µì ì¸ íŒ¨í„´ ì œê±°
                    if not re.match(r'^(.+?)\1+$', text):  # ê°™ì€ ë¬¸ì¥ ë°˜ë³µ ì œê±°
                        cleaned.append(text)
        
        return list(set(cleaned))
    
    def _is_valid_board_review(self, text):
        """ê²Œì‹œíŒ ë¦¬ë·°ê°€ ìœ íš¨í•œì§€ í™•ì¸ (í™•ì¥ëœ ë¦¬ë·°ìš©)"""
        if not text or len(text) < 10 or len(text) > 1000:
            return False
        
        # ì œì™¸í•  íŒ¨í„´ë“¤
        exclude_patterns = [
            r'ë² ìŠ¤íŠ¸.*ë¦¬ë·°',
            r'ì¡°íšŒ\s*\d+$',
            r'^\d+$',
            r'^í˜ì´ì§€',
            r'ì´ì „.*ë‹¤ìŒ',
            r'ê²€ìƒ‰.*ì¡°ê±´',
            r'ì „ì²´.*ìƒí’ˆ',
            r'ì¹´í…Œê³ ë¦¬',
            r'ì •ë ¬.*ìˆœì„œ'
        ]
        
        for pattern in exclude_patterns:
            if re.search(pattern, text):
                return False
        
        # ë¦¬ë·°ë‹¤ìš´ íŒ¨í„´ í™•ì¸
        review_indicators = [
            r'ì¢‹ì•„ìš”|ì¢‹ë„¤ìš”|ë§›ìˆ|ì¶”ì²œ|ë§Œì¡±|ì˜.*ì‚°|í¸í•´ìš”|ê¹”ë”',  # ê¸ì •ì 
            r'ë¹„ì‹¸ìš”|ê¹¨ì ¸|ë¬¸ì œ|ë¶ˆí¸|ì‹¤ë§|ë³„ë¡œ|ì•ˆ.*ì¢‹',  # ë¶€ì •ì 
            r'ì‚¬ìš©|êµ¬ë§¤|ë°°ì†¡|í¬ì¥|ì œí’ˆ|ìƒí’ˆ|ë¦¬ë·°',  # ì¼ë°˜ì 
            r'ì •ë§|ë„ˆë¬´|ê½¤|ì•„ì£¼|ë§¤ìš°|ì™„ì „'  # ê°•ì¡° í‘œí˜„
        ]
        
        has_review_indicator = any(re.search(pattern, text) for pattern in review_indicators)
        
        # í•œê¸€ì´ í¬í•¨ë˜ì–´ì•¼ í•¨
        if not re.search(r'[ê°€-í£]{3,}', text):
            return False
        
        return has_review_indicator
    
    def crawl_reviews(self, url):
        """URLì— ë”°ë¼ ì ì ˆí•œ í¬ë¡¤ëŸ¬ ì„ íƒ"""
        mall_type = self.detect_mall_type(url)
        
        print(f"Detected mall type: {mall_type}")
        
        if mall_type == 'coupang':
            return self.crawl_coupang(url)
        elif mall_type == 'naver':
            return self.crawl_naver(url)
        elif mall_type == '11st':
            return self.crawl_11st(url)
        elif mall_type == 'dentiste':
            return self.crawl_dentiste(url)
        elif mall_type == 'dentiste_board':
            return self.crawl_dentiste_board(url)
        else:
            return self.crawl_generic(url)
    
    def extract_product_info(self, url):
        """ì œí’ˆ ì •ë³´ ì¶”ì¶œ"""
        try:
            response = requests.get(url, headers=self.headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œí’ˆëª… ì¶”ì¶œ ì‹œë„
            title = None
            title_selectors = [
                'h1', 'h2',
                {'class': re.compile('product.*title', re.I)},
                {'class': re.compile('item.*name', re.I)},
                {'id': re.compile('product.*title', re.I)}
            ]
            
            for selector in title_selectors:
                if isinstance(selector, dict):
                    element = soup.find(attrs=selector)
                else:
                    element = soup.find(selector)
                    
                if element:
                    title = element.get_text(strip=True)
                    break
            
            # ê°€ê²© ì¶”ì¶œ ì‹œë„
            price = None
            price_selectors = [
                {'class': re.compile('price', re.I)},
                {'class': re.compile('cost', re.I)},
                {'class': re.compile('amount', re.I)}
            ]
            
            for selector in price_selectors:
                element = soup.find(attrs=selector)
                if element:
                    price_text = element.get_text(strip=True)
                    # ìˆ«ìë§Œ ì¶”ì¶œ
                    price_match = re.search(r'[\d,]+', price_text)
                    if price_match:
                        price = price_match.group()
                        break
            
            return {
                'title': title or 'Unknown Product',
                'price': price,
                'url': url
            }
            
        except Exception as e:
            print(f"Product info extraction error: {e}")
            return {
                'title': 'Unknown Product',
                'price': None,
                'url': url
            }